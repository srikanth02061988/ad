def generate_responses(question, relevant_docs, client, deployment="ajitTest", num_responses=3):
    try:
        max_tokens = 4096  # Adjust based on your model's maximum context length
        combined_docs = "\n\n".join(relevant_docs)
        
        # Calculate the available tokens for the input text
        question_tokens = len(question.split())
        remaining_tokens = max_tokens - question_tokens - 100  # Reserve tokens for the response and prompt structure

        # Trim the combined_docs to fit within the remaining token limit
        doc_tokens = combined_docs.split()
        if len(doc_tokens) > remaining_tokens:
            combined_docs = " ".join(doc_tokens[:remaining_tokens])

        prompt = f"Based on the following documents, answer the question: {question}\n\nDocuments:\n\n{combined_docs}"

        responses = []
        for _ in range(num_responses):
            response = client.chat.completions.create(
                model=deployment,
                messages=[
                    {"role": "system", "content": "Assistant is a large language model trained by OpenAI."},
                    {"role": "user", "content": prompt}
                ]
            )
            text_response = response.choices[0].message.content.strip()
            responses.append(text_response)
        logger.info("Responses generated successfully.")
        return responses
    except Exception as e:
        logger.error(f"An error occurred while generating responses: {e}")
        st.error(f"An error occurred while generating responses: {e}")
        return []

