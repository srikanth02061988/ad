# Step 1: Write initial results with event_timestamp
results_df = results_df.withColumnRenamed("timestamp", "event_timestamp")

results_df.select(
    "event_timestamp", 
    "ssc_RAI_session_id", 
    "ssc_RAI_chainid", 
    "ssc_RAI_appcode", 
    "hallucination_score", 
    "is_hallucination"
).write.format("delta").option("mergeSchema", "true").mode("overwrite").save("dbfs:/user/hive/warehouse/rai_payload_synth_hallucination_results")

print("Saved results to Delta table with 'event_timestamp'.")

# Step 2: Read the table back
delta_table = spark.read.format("delta").load("dbfs:/user/hive/warehouse/rai_payload_synth_hallucination_results")

# Step 3: Rename event_timestamp back to timestamp and drop duplicates if any
final_df = delta_table.withColumnRenamed("event_timestamp", "timestamp")

# Save the final results back to the Delta table
final_df.write.format("delta").option("mergeSchema", "true").mode("overwrite").save("dbfs:/user/hive/warehouse/rai_payload_synth_hallucination_results")

print("Final results saved to Delta table with 'timestamp'.")
