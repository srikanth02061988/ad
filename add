import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import psycopg2
from psycopg2.extras import execute_values
import numpy as np
import logging
import os
import requests
import json
import uuid

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set up Azure OpenAI credentials
azure_endpoint = os.getenv("EFF_OAI_BASE", "")
azure_api_key = os.getenv("EFF_OAI_KEY", "")
embedding_deployment = "TEB-Robin"  # Embeddings deployment
completion_deployment = "GPT4_robin"  # Completions deployment

# Define headers for Azure OpenAI API requests
headers = {
    "Content-Type": "application/json",
    "api-key": azure_api_key
}

def get_embeddings(text, endpoint, headers, deployment=embedding_deployment):
    try:
        logger.info(f"Generating embeddings for the text chunk: {text[:30]}...")
        url = f"{endpoint}/openai/deployments/{deployment}/embeddings?api-version=2024-02-15-preview"
        data = {"input": text}
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        result = response.json()
        embedding = result['data'][0]['embedding']
        logger.info("Embeddings generated successfully.")
        return embedding
    except requests.exceptions.RequestException as e:
        logger.error(f"Error generating embeddings: {e}")
        st.error(f"An error occurred while generating embeddings: {e}")
        return None

def get_pdf_text(pdf_docs):
    text = ""
    try:
        logger.info("Extracting text from PDF...")
        reader = PdfReader(pdf_docs)
        for page in reader.pages:
            text += page.extract_text()
        logger.info("Text extraction from PDF successful.")
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {e}")
        st.error(f"An error occurred while extracting text from PDF: {e}")
    return text

def get_text_chunks(text):
    logger.info("Splitting text into chunks...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
    chunks = text_splitter.split_text(text)
    logger.info(f"Text split into {len(chunks)} chunks.")
    return chunks

def store_embeddings_in_db(pdf_id, text_chunks, endpoint, headers):
    try:
        logger.info("Connecting to PostgreSQL database...")
        conn = psycopg2.connect(
            host=os.getenv("DB_HOST"),
            port=os.getenv("DB_PORT"),
            database=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD")
        )

        with conn.cursor() as cur:
            logger.info("Creating table if it does not exist...")
            cur.execute("""
            CREATE TABLE IF NOT EXISTS embeddings (
                id BIGSERIAL PRIMARY KEY,
                pdf_id UUID,
                content TEXT,
                embedding vector(1536)  -- Store embeddings as vector with 1536 dimensions
            );
            """)
            conn.commit()

        embeddings = []
        for chunk in text_chunks:
            logger.info(f"Generating embedding for chunk: {chunk[:30]}...")
            embedding = get_embeddings(chunk, endpoint, headers)
            if embedding is not None:
                embedding_vector = list(map(float, embedding))  # Ensure the embedding is a list of floats
                embeddings.append((pdf_id, chunk, embedding_vector))  # Store as list
            else:
                logger.error("Failed to generate embedding for a chunk, skipping it.")

        if embeddings:
            with conn.cursor() as cur:
                logger.info("Inserting embeddings into the database...")
                execute_values(cur, "INSERT INTO embeddings (pdf_id, content, embedding) VALUES %s", embeddings)
                conn.commit()
        else:
            logger.error("No embeddings generated to store in the database.")

        conn.close()
        logger.info("Embeddings stored in the database successfully.")
    except Exception as e:
        logger.error(f"Error storing embeddings in database: {e}")
        st.error(f"An error occurred while storing embeddings in the database: {e}")

def retrieve_embeddings_from_db(pdf_id):
    try:
        logger.info("Connecting to PostgreSQL database for retrieval...")
        conn = psycopg2.connect(
            host=os.getenv("DB_HOST"),
            port=os.getenv("DB_PORT"),
            database=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD")
        )

        with conn.cursor() as cur:
            logger.info(f"Retrieving embeddings for PDF ID: {pdf_id}...")
            cur.execute("SELECT content, embedding::text FROM embeddings WHERE pdf_id = %s", (pdf_id,))
            rows = cur.fetchall()
        conn.close()

        if rows:
            docs = [row[0] for row in rows]
            embeddings = [np.array(list(map(float, row[1][1:-1].split(','))), dtype=np.float32) for row in rows]  # Convert to numpy array
            return docs, embeddings
        else:
            logger.info("No embeddings found for the given PDF ID.")
            return None, None
    except Exception as e:
        logger.error(f"Error retrieving embeddings from database: {e}")
        st.error(f"An error occurred while retrieving embeddings from the database: {e}")
        return None, None

def find_relevant_chunks(question, docs, embeddings, top_k=5):
    # Dummy implementation, replace with actual logic to find relevant chunks
    return docs[:top_k]

def generate_responses(question, relevant_docs, endpoint, headers, deployment=completion_deployment):
    try:
        logger.info("Generating responses for the question...")
        combined_docs = "\n\n".join(relevant_docs)
        prompt = f"Based on the following documents, answer the question: {question}\n\nDocuments:\n{combined_docs}"
        
        url = f"{endpoint}/openai/deployments/{deployment}/completions?api-version=2023-05-15-preview"
        data = {
            "prompt": prompt,
            "max_tokens": 50  # Adjust this based on your requirements
        }
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        result = response.json()
        text_response = result['choices'][0]['text'].strip()
        
        return text_response
    except requests.exceptions.RequestException as e:
        logger.error(f"Error generating responses: {e}")
        st.error(f"An error occurred while generating responses: {e}")
        return None

def main():
    st.set_page_config(page_title="Digital Marketing")
    st.header("Generate Derivative Content")

    st.sidebar.title("Menu:")
    
    pdf_docs = st.sidebar.file_uploader("Upload your PDF Files and Click on the submit and process", type=["pdf"])
    if st.sidebar.button("Submit & Process"):
        if pdf_docs is not None:
            with st.spinner("Processing..."):
                raw_text = get_pdf_text(pdf_docs)
                if raw_text:
                    text_chunks = get_text_chunks(raw_text)
                    pdf_id = str(uuid.uuid4())
                    store_embeddings_in_db(pdf_id, text_chunks, azure_endpoint, headers)
                    st.session_state["pdf_id"] = pdf_id  # Store PDF ID in session state
                else:
                    logger.error("No text extracted from PDF.")
                    st.error("Failed to extract text from the uploaded PDF.")
            st.sidebar.success("Done")

    if "pdf_id" in st.session_state:
        category = st.selectbox("Select the category to search within:", ["Twitter", "LinkedIn", "Other"])
        user_question = st.text_input("Ask a Question based on the uploaded PDF")
        if user_question and st.button("Submit Question"):
            with st.spinner("Retrieving and processing..."):
                pdf_id = st.session_state["pdf_id"]
                docs, embeddings = retrieve_embeddings_from_db(pdf_id)
                if docs and embeddings:
                    relevant_docs = find_relevant_chunks(user_question, docs, embeddings)
                    text_response = generate_responses(user_question, relevant_docs, azure_endpoint, headers)
                    if text_response:
                        st.write("Response:")
                        st.write(text_response)
                        st.download_button("Download Response", data=text_response, file_name="response.txt")
                    else:
                        st.error("Failed to generate responses.")
                else:
                    st.error("No relevant embeddings found for the uploaded PDF.")

if __name__ == "__main__":
    main()
