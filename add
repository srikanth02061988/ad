import json
import datetime
from openai import AzureOpenAI
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.sql.types import TimestampType

# Initialize Spark session
spark = SparkSession.builder \
    .appName("MetricsDetection") \
    .getOrCreate()

# Initialize Azure OpenAI client
client = AzureOpenAI(
    api_version="2024-02-01",
    azure_endpoint="https://effopenai.openai.azure.com/",
    api_key="7896c56d537d4f19803a4ac85bd9bc9"
)

def call_llm_model(prompt):
    """Calls the LLM model with the given prompt and returns the response."""
    try:
        response = client.chat.completions.create(
            model="ajitTest",
            messages=[
                {"role": "system", "content": "Assistant is a large language model trained by OpenAI."},
                {"role": "user", "content": prompt}
            ]
        )
        output = response.choices[0].message.content.strip()
        return output
    except Exception as e:
        print(f"Error calling LLM model: {e}")
        return None

def generate_sample_outputs(reference_text, input_text, num_samples=5):
    """Generates sample outputs using the LLM model."""
    sample_outputs = []
    prompt = f"Reference: {reference_text}\nInput: {input_text}\nPlease provide a response based on the reference and input."
    
    for _ in range(num_samples):
        sample_output = call_llm_model(prompt)
        if sample_output:
            sample_outputs.append(sample_output)
    print(f"Generated {num_samples} sample outputs.")
    return sample_outputs

def consistency_check(reference_text, input_text, original_output, sample_output):
    """Checks the consistency between the original and sample outputs."""
    prompt = f"""
    Reference: {reference_text}
    Input: {input_text}
    Original Output: {original_output}
    Sample Output: {sample_output}
    Does the sample output support the original output? Answer "Yes" or "No" and provide a brief explanation.
    """
    response = call_llm_model(prompt)
    return response

def detect_closed_domain_hallucination(reference_text, input_text, original_output, num_samples=5, session_id=None, chain_id=None, app_code=None):
    """Detects closed-domain hallucinations in the LLM outputs."""
    sample_outputs = generate_sample_outputs(reference_text, input_text, num_samples)
    consistency_responses = []
    consistency_scores = []

    for sample_output in sample_outputs:
        response = consistency_check(reference_text, input_text, original_output, sample_output)
        if response:
            consistency_responses.append(response)
            consistency_scores.append(0 if "yes" in response.lower() else 1)

    # Calculate the hallucination score
    hallucination_score = sum(consistency_scores) / num_samples if num_samples > 0 else 0
    is_hallucination = hallucination_score > 0.5

    results = {
        "timestamp": datetime.datetime.now().isoformat(),
        "ssc_RAI_session_id": session_id,
        "ssc_RAI_chainid": chain_id,
        "ssc_RAI_appcode": app_code,
        "hallucination_score": hallucination_score,
        "is_hallucination": is_hallucination
    }

    print(f"Hallucination detection results: {results}")
    return results

def detect_groundedness(reference_text, input_text, original_output, num_samples=5, session_id=None, chain_id=None, app_code=None):
    """Detects groundedness in the LLM outputs."""
    sample_outputs = generate_sample_outputs(reference_text, input_text, num_samples)
    groundedness_responses = []
    groundedness_scores = []

    for sample_output in sample_outputs:
        prompt = f"""
        Reference: {reference_text}
        Input: {input_text}
        Original Output: {original_output}
        Sample Output: {sample_output}
        Is the sample output grounded in the reference text? Answer "Yes" or "No" and provide a brief explanation.
        """
        response = call_llm_model(prompt)
        if response:
            groundedness_responses.append(response)
            groundedness_scores.append(0 if "yes" in response.lower() else 1)

    # Calculate the groundedness score
    groundedness_score = sum(groundedness_scores) / num_samples if num_samples > 0 else 0
    is_grounded = groundedness_score > 0.5

    results = {
        "timestamp": datetime.datetime.now().isoformat(),
        "ssc_RAI_session_id": session_id,
        "ssc_RAI_chainid": chain_id,
        "ssc_RAI_appcode": app_code,
        "groundedness_score": groundedness_score,
        "is_grounded": is_grounded
    }

    print(f"Groundedness detection results: {results}")
    return results

# Read the Delta table
delta_table = spark.read.format("delta").load("dbfs:/FileStore/synthetic_data_new_hallucination_v5.json")
print("Read Delta table.")

# Print the schema of the DataFrame
delta_table.printSchema()
print("Printed schema of the DataFrame.")

# Explode the nested structure to access the necessary fields
df = delta_table.select(
    "timestamp",
    "ssc_RAI_application_attributes.ssc_RAI_appcode",
    "ssc_RAI_application_attributes.ssc_RAI_session_id",
    "ssc_RAI_application_attributes.ssc_RAI_chainid",
    "ssc_RAI_requestResponse_attributes.ssc_RAI_attribute",
    "ssc_RAI_requestResponse_attributes.ssc_RAI_prompt",
    "ssc_RAI_requestResponse_attributes.ssc_RAI_context",
    "ssc_RAI_requestResponse_attributes.ssc_RAI_groundingData"
)
print("Exploded nested structures.")

# Separate DataFrame into inputs, outputs, and contexts
inputs_df = df.filter(F.col("ssc_RAI_attribute") == "input")
outputs_df = df.filter(F.col("ssc_RAI_attribute") == "output")
contexts_df = df.filter(F.col("ssc_RAI_attribute") == "context")
print("Filtered inputs, outputs, and contexts.")

# Join inputs and outputs on session ID, chain ID, and app code
joined_df = inputs_df.alias("inputs").join(
    outputs_df.alias("outputs"),
    (F.col("inputs.ssc_RAI_session_id") == F.col("outputs.ssc_RAI_session_id")) &
    (F.col("inputs.ssc_RAI_chainid") == F.col("outputs.ssc_RAI_chainid")) &
    (F.col("inputs.ssc_RAI_appcode") == F.col("outputs.ssc_RAI_appcode"))
).join(
    contexts_df.alias("contexts"),
    (F.col("inputs.ssc_RAI_session_id") == F.col("contexts.ssc_RAI_session_id")) &
    (F.col("inputs.ssc_RAI_chainid") == F.col("contexts.ssc_RAI_chainid")) &
    (F.col("inputs.ssc_RAI_appcode") == F.col("contexts.ssc_RAI_appcode"))
)
print("Joined inputs, outputs, and contexts.")

# Select the relevant columns
data_df = joined_df.select(
    F.col("inputs.ssc_RAI_prompt").alias("input"),
    F.col("contexts.ssc_RAI_context").alias("reference"),
    F.col("outputs.ssc_RAI_prompt").alias("output"),
    F.col("inputs.ssc_RAI_session_id"),
    F.col("inputs.ssc_RAI_chainid"),
    F.col("inputs.ssc_RAI_appcode")
)
print("Selected relevant columns.")

# Convert DataFrame to Pandas for processing
data = data_df.toPandas().to_dict('records')
print("Converted DataFrame to Pandas.")

num_samples = 5
all_results = []

for row in data:
    reference_text = row.get('reference')
    input_prompt = row.get('input')
    original_output = row.get('output')
    session_id = row.get('ssc_RAI_session_id')
    chain_id = row.get('ssc_RAI_chainid')
    app_code = row.get('ssc_RAI_appcode')

    if reference_text and input_prompt and original_output:
        hallucination_results = detect_closed_domain_hallucination(reference_text, input_prompt, original_output, num_samples, session_id, chain_id, app_code)
        groundedness_results = detect_groundedness(reference_text, input_prompt, original_output, num_samples, session_id, chain_id, app_code)
        combined_results = {**hallucination_results, **groundedness_results}
        all_results.append(combined_results)
        print(f"Processed data for session ID: {session_id}, chain ID: {chain_id}, app code: {app_code}.")
    else:
        print(f"Skipped data for session ID: {session_id}, chain ID: {chain_id}, app code: {app_code} due to missing output.")

# Convert results to DataFrame
results_df = spark.createDataFrame(all_results)
print("Converted results to DataFrame.")

# Ensure the timestamp column is of the correct type
results_df = results_df.withColumn("timestamp", F.col("timestamp").cast(TimestampType()))

# Save results to Delta table with schema merging enabled
results_df.select(
    "timestamp", 
    "ssc_RAI_session_id", 
    "ssc_RAI_chainid", 
    "ssc_RAI_appcode", 
    "hallucination_score", 
    "is_hallucination",
    "groundedness_score",
    "is_grounded"
).write.format("delta").option("mergeSchema", "true").mode("overwrite").save("dbfs:/user/hive/warehouse/rai_payload_synth_metrics_results")

print("Saved results to Delta table with 'timestamp'.")

