import json
import datetime
from openai import AzureOpenAI
import pyspark.sql.functions as F
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("HallucinationDetection") \
    .getOrCreate()

# Initialize Azure OpenAI client
client = AzureOpenAI(
    api_version="2024-02-01",
    azure_endpoint="https://effopenai.openai.azure.com/",
    api_key="7896c56d537d4f19803a4ac85bd9bc9"
)

def call_llm_model(prompt):
    try:
        print(f"Calling LLM model with prompt: {prompt}")
        response = client.chat.completions.create(
            model="ajitTest",
            messages=[
                {"role": "system", "content": "Assistant is a large language model trained by OpenAI."},
                {"role": "user", "content": prompt}
            ]
        )
        output = response.choices[0].message.content.strip()
        print(f"Received response: {output}")
        return output
    except Exception as e:
        print(f"Error calling LLM model: {e}")
        return None

def generate_sample_outputs(reference_text, input_text, num_samples=5):
    sample_outputs = []
    prompt = f"Reference: {reference_text}\nInput: {input_text}\nPlease provide a response based on the reference and input."

    print(f"Generating {num_samples} sample outputs...")
    for _ in range(num_samples):
        sample_output = call_llm_model(prompt)
        if sample_output:
            sample_outputs.append(sample_output)
    print(f"Generated sample outputs: {sample_outputs}")
    return sample_outputs

def consistency_check(reference_text, input_text, original_output, sample_output):
    prompt = f"""
    Reference: {reference_text}
    Input: {input_text}
    Original Output: {original_output}
    Sample Output: {sample_output}
    Does the sample output support the original output? Answer "Yes" or "No" and provide a brief explanation.
    """
    response = call_llm_model(prompt)
    return response

def detect_closed_domain_hallucination(reference_text, input_text, original_output, num_samples=5, session_id=None, chain_id=None, app_code=None):
    print(f"Detecting hallucination for input: {input_text}")
    sample_outputs = generate_sample_outputs(reference_text, input_text, num_samples)
    consistency_responses = []
    consistency_scores = []

    for sample_output in sample_outputs:
        response = consistency_check(reference_text, input_text, original_output, sample_output)
        if response:
            consistency_responses.append(response)
            consistency_scores.append(0 if "yes" in response.lower() else 1)

    # Calculate the hallucination score
    hallucination_score = sum(consistency_scores) / num_samples if num_samples > 0 else 0
    is_hallucination = hallucination_score > 0.5

    results = {
        "timestamp": datetime.datetime.now().isoformat(),
        "ssc_RAI_session_id": session_id,
        "ssc_RAI_chainid": chain_id,
        "ssc_RAI_appcode": app_code,
        "hallucination_score": hallucination_score,
        "is_hallucination": is_hallucination
    }

    print(f"Detection results: {results}")
    return results

# Read the Delta table
print("Reading Delta table...")
delta_table = spark.read.format("delta").load("/path/to/delta/table")

# Filter the Delta table for input and output attributes
print("Filtering inputs and outputs...")
inputs_df = delta_table.filter(F.col("ssc_RAI_attribute") == "input")
outputs_df = delta_table.filter(F.col("ssc_RAI_attribute") == "output")

# Join the inputs and outputs on session ID, chain ID, and app code
print("Joining inputs and outputs...")
joined_df = inputs_df.alias("inputs").join(
    outputs_df.alias("outputs"),
    (F.col("inputs.ssc_RAI_session_id") == F.col("outputs.ssc_RAI_session_id")) &
    (F.col("inputs.ssc_RAI_chainid") == F.col("outputs.ssc_RAI_chainid")) &
    (F.col("inputs.ssc_RAI_appcode") == F.col("outputs.ssc_RAI_appcode"))
)

# Select the relevant columns
print("Selecting relevant columns...")
data_df = joined_df.select(
    F.col("inputs.ssc_RAI_prompt").alias("input"),
    F.col("inputs.ssc_RAI_context").alias("reference"),
    F.col("outputs.ssc_RAI_prompt").alias("output"),
    F.col("inputs.ssc_RAI_session_id"),
    F.col("inputs.ssc_RAI_chainid"),
    F.col("inputs.ssc_RAI_appcode")
)

# Convert DataFrame to Pandas for processing
print("Converting DataFrame to Pandas...")
data = data_df.toPandas().to_dict('records')

num_samples = 5
all_results = []

print("Processing data...")
for row in data:
    reference_text = row.get('reference')
    input_prompt = row.get('input')
    original_output = row.get('output')
    session_id = row.get('ssc_RAI_session_id')
    chain_id = row.get('ssc_RAI_chainid')
    app_code = row.get('ssc_RAI_appcode')

    if reference_text and input_prompt and original_output:
        results = detect_closed_domain_hallucination(reference_text, input_prompt, original_output, num_samples, session_id, chain_id, app_code)
        all_results.append(results)
        print(f"Reference: {reference_text}")
        print(f"Input: {input_prompt}")
        print(f"Original Output: {original_output}")
        print(f"Session ID: {session_id}")
        print(f"Chain ID: {chain_id}")
        print(f"App Code: {app_code}")
        print("="*80)

# Convert results to DataFrame
print("Converting results to DataFrame...")
results_df = spark.createDataFrame(all_results)

# Save results to Delta table
print("Saving results to Delta table...")
results_df.select("timestamp", "ssc_RAI_session_id", "ssc_RAI_chainid", "ssc_RAI_appcode", "hallucination_score", "is_hallucination") \
          .write.format("delta").mode("overwrite").save("/path/to/save/results")

print("Results saved to Delta table")

